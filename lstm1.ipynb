{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_tokenize(sequence):\n",
    "    return list(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, vocab_dim, embedding_dim, hidden_dim, num_layer):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_dim, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layer, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, vocab_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2tensor(sequence, token2idx, input_len, vocab):\n",
    "    new_sequence = [token2idx['<BOS>'], ]\n",
    "    mask = [1, ]\n",
    "    seq_len = len(sequence)\n",
    "    for i in range(input_len - 2):\n",
    "        is_end = True\n",
    "        if i < seq_len - 3:\n",
    "            if sequence[i] in vocab:\n",
    "                new_sequence.append(token2idx[sequence[i]])\n",
    "                mask.append(1)\n",
    "            else:\n",
    "                new_sequence.append(token2idx['<UNK>'])\n",
    "                mask.append(0)\n",
    "        else:\n",
    "            if is_end:\n",
    "                mask.append(1)\n",
    "                is_end = False\n",
    "            else:\n",
    "                mask.append(0)\n",
    "\n",
    "            new_sequence.append(token2idx['<EOS>'])\n",
    "    \n",
    "    if seq_len > input_len:\n",
    "        mask.append(1)\n",
    "    else:\n",
    "        mask.append(0)\n",
    "    new_sequence.append(token2idx['<EOS>'])\n",
    "\n",
    "    new_sequence = torch.tensor(new_sequence)\n",
    "    mask = torch.tensor(mask)\n",
    "    assert len(new_sequence) == input_len, f'{len(new_sequence), input_len, seq_len}'\n",
    "    return new_sequence, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, token2idx, input_len, vocab):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.token2idx = token2idx\n",
    "        self.input_len = input_len\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.data.iloc[idx, 0]\n",
    "        sequence = self.tokenizer(sequence)\n",
    "        sequence, mask = text2tensor(sequence, self.token2idx, self.input_len, self.vocab)\n",
    "        return sequence[:-1], sequence[1:], mask[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(data, ban_words, new_words, vocab_size, tokenizer, threshold):\n",
    "    vocabulary = {new_word: 1 for new_word in new_words}\n",
    "    filtered_vocabulary = new_words\n",
    "    count = len(vocabulary)\n",
    "    for frame in tqdm(data):\n",
    "        for i in trange(len(frame)):\n",
    "            sequence = frame.iloc[i, 0]\n",
    "            for token in tokenizer(sequence):\n",
    "                if token not in ban_words and count < vocab_size:\n",
    "                    if token not in vocabulary:\n",
    "                        vocabulary[token] = 1\n",
    "                    else:\n",
    "                        vocabulary[token] += 1\n",
    "                    count += 1\n",
    "\n",
    "    for token, count in vocabulary.items():\n",
    "        if count > threshold:\n",
    "           filtered_vocabulary.append(token)\n",
    "\n",
    "    print(f'Vocabulary len={len(filtered_vocabulary)}')\n",
    "    return filtered_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_device(model, device, parallel_type):\n",
    "    if device == 'cpu':\n",
    "        model = model.to(device)\n",
    "        print(f'device={device}')\n",
    "    else:\n",
    "        if parallel_type == 'ddp':\n",
    "            model = torch.nn.parallel.DistributedDataParallel(model)\n",
    "            print(f'devices={list(range(torch.cuda.device_count()))}')\n",
    "        elif parallel_type == 'dp':\n",
    "            model = torch.nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\n",
    "            model = model.to(device)\n",
    "            print(f'devices={list(range(torch.cuda.device_count()))}')\n",
    "        elif parallel_type is None:\n",
    "            print(f'devices={[0]}')\n",
    "            model = model.to(device)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_training(losses, best_epoch):\n",
    "    print(f'Training is complete, best epoch: {best_epoch}')\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    axs[0].plot(losses['train'], label='Train Loss')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].set_ylabel('Loss value')\n",
    "    axs[0].grid()\n",
    "    axs[0].legend()\n",
    "\n",
    "    axs[1].plot(losses['valid'], label='Valid Loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_ylabel('Loss value')\n",
    "    axs[1].grid()\n",
    "    axs[1].legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, dataloader, loss_func, optimizer, parallel_type, device, early_stopping, scheduler=None, epochs=10):\n",
    "    losses = {\n",
    "        'train': [],\n",
    "        'valid': []\n",
    "    }\n",
    "    best_loss = np.inf\n",
    "    best_epoch = 0\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for phase in ('train', 'valid'):\n",
    "            total_loss = 0\n",
    "            for batch in tqdm(dataloader[phase]):\n",
    "                inputs, labels, mask = batch\n",
    "                inputs, labels, mask = inputs.to(device), labels.to(device), mask.to(device)\n",
    "                \n",
    "                model.train(phase == 'train')\n",
    "                if phase == 'train':\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    loss = loss_func(outputs, labels)\n",
    "                    loss = (loss * mask).sum()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(inputs)\n",
    "                        loss = loss_func(outputs, labels)\n",
    "                        loss = (loss * mask).sum()\n",
    "\n",
    "                batch_loss = loss.item() / mask.sum()\n",
    "                losses[phase].append(batch_loss.detach().cpu().numpy())\n",
    "                total_loss += batch_loss\n",
    "            \n",
    "\n",
    "            epoch_loss = total_loss / len(dataloader)\n",
    "\n",
    "            if phase =='train' and scheduler is not None:\n",
    "                scheduler.step()\n",
    "\n",
    "            if phase == 'valid':\n",
    "                print('Epoch', epoch+1)\n",
    "                print('Train Loss:', losses['train'][-1])\n",
    "                print('Valid Loss:', losses['valid'][-1])\n",
    "                if epoch_loss < best_loss:\n",
    "                    best_loss = epoch_loss\n",
    "                    best_model = model\n",
    "                    best_epoch = epoch\n",
    "                    print('Best score!')\n",
    "                if epoch - best_epoch == early_stopping:\n",
    "                    save_training(\n",
    "                        losses, best_epoch\n",
    "                    )\n",
    "                    if parallel_type == None:\n",
    "                        return best_model\n",
    "                    else:\n",
    "                        return best_model.module\n",
    "\n",
    "                print()\n",
    "\n",
    "    save_training(\n",
    "        losses, best_epoch\n",
    "    )\n",
    "\n",
    "    if parallel_type == None:\n",
    "        return best_model\n",
    "    else:\n",
    "        return best_model.module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    np.random.RandomState(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, T=1.0, eps=1e-8):\n",
    "    if T == 1:\n",
    "        x_sum = x.sum()\n",
    "        return x / x_sum\n",
    "    else:\n",
    "        x_exp = np.exp(x / T + eps)\n",
    "        x_sum = x_exp.sum()\n",
    "        return x_exp / x_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(best_model, prompt,  tokenizer, token2idx, idx2token, vocabulary, device, generation_len=100, T=1, use_argmax=True):\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        while True:\n",
    "            sequence = tokenizer(prompt)\n",
    "            sequence, mask = text2tensor(prompt, token2idx, len(sequence), vocabulary)\n",
    "            sequence = sequence[:-1].to(device)\n",
    "            sequence = sequence.unsqueeze(0)\n",
    "            pred = best_model(sequence).squeeze(0).cpu().numpy()[:, -1]\n",
    "            probs = softmax(pred, T=T)\n",
    "            if use_argmax:\n",
    "                next_token = idx2token[probs.argmax(0)]\n",
    "            else:\n",
    "                next_token = np.random.choice(vocabulary, p=probs)\n",
    "\n",
    "            prompt = prompt + next_token\n",
    "\n",
    "            if next_token == '<EOS>' or len(prompt) > generation_len:\n",
    "                break\n",
    "\n",
    "    return prompt        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = r'/root/storage/3030/AkhmetzyanovD/projects/mai/cnn_dailymail/train.csv'\n",
    "valid_path = r'/root/storage/3030/AkhmetzyanovD/projects/mai/cnn_dailymail/valid.csv'\n",
    "train_data = pd.read_csv(train_path, usecols=['article'])\n",
    "valid_data = pd.read_csv(valid_path, usecols=['article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.head(int(len(train_data) / 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "parallel_type = 'dp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'This will create a '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## single layer LSTM with char tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ban_words = ['\\t', '\\n', '\\u2009', '\\xa0', '|', '´', '’',  '‘', '–', 'ñ', 'â' , '“', '½', 'Î', 'í', 'ó', 'Â', 'Ã', 'ﬁ', '¿', '•', 'É', 'è', 'š', 'ū', 'ė', 'č', '¢', 'é', 'ô']\n",
    "new_words = ['<EOS>', '<BOS>', '<UNK>']\n",
    "vocabulary = ['<EOS>', '<BOS>', '<UNK>', 'B', 'y', ' ', '.', 'A', 's', 'o', 'c', 'i', 'a', 't', 'e', 'd', 'P', 'r', 'U', 'L', 'I', 'S', 'H', 'E', 'D', ':', '1', '4', 'T', ',', '2', '5', 'O', 'b', '0', '3', '6', 'h', 'p', 'f', 'F', 'g', 'C', 'l', 'n', 'N', 'k', 'x', 'u', 'm', 'G', 'J', 'w', 'v', '(', ')', 'z', 'M', \"'\", '-', 'R', '\"', '9', 'K', 'W', 'j', '7', '8', '[', ']', 'Y', 'q', 'V', '$', '%', 'Q', '£', '/', '!', 'Z', '¬', '#', ';', '©', '&', '@', '_', '?', '*', 'X', '—', '+', '…', '»', '”', '€', '<', '{', '}']\n",
    "vocab_size = 99\n",
    "# vocabulary =  get_vocabulary([train_data, valid_data], ban_words, new_words, vocab_size, word_tokenize, 0)\n",
    "vocab_size = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2idx = {token: i for i, token in enumerate(vocabulary)}\n",
    "idx2token = {i: token for i, token in enumerate(vocabulary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(train_data, char_tokenize, token2idx, input_len=512, vocab=vocabulary)\n",
    "valid_dataset = TextDataset(valid_data, char_tokenize, token2idx, input_len=512, vocab=vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=1408, shuffle=False)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=1408, shuffle=False)\n",
    "dataloaders = {\n",
    "    'train': train_dataloader,\n",
    "    'valid': valid_dataloader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleLSTM(vocab_dim=vocab_size, embedding_dim=300, hidden_dim=128, num_layer=1)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "scheduler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = choose_device(model, device, parallel_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = train_loop(\n",
    "    model=model, \n",
    "    dataloader=dataloaders, \n",
    "    loss_func=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    parallel_type=parallel_type, \n",
    "    device=device, \n",
    "    early_stopping=10, \n",
    "    scheduler=scheduler, \n",
    "    epochs=3\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = test(\n",
    "    best_model, \n",
    "    prompt, \n",
    "    char_tokenize, \n",
    "    token2idx, \n",
    "    idx2token, \n",
    "    vocabulary, \n",
    "    device, \n",
    "    generation_len=100, \n",
    "    T=1, \n",
    "    use_argmax=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## single layer LSTM with word tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ban_words = ['\\t', '\\n', '\\u2009', '\\xa0', '(CNN)']\n",
    "new_words = ['<EOS>', '<BOS>', '<UNK>']\n",
    "vocabulary = ['<EOS>', '<BOS>', '<UNK>', 'By', '.', 'Associated', 'Press', 'PUBLISHED', ':', '14:11', 'EST', ',', '25', 'October', '2013', '|', 'UPDATED', '15:36', 'The', 'bishop', 'of', 'the', 'Fargo', 'Catholic', 'Diocese', 'in', 'North', 'Dakota', 'has', 'exposed', 'potentially', 'hundreds', 'church', 'members', 'Grand', 'Forks', 'and', 'Jamestown', 'to', 'hepatitis', 'A', 'virus', 'late', 'September', 'early', 'state', 'Health', 'Department', 'issued', 'an', 'advisory', 'exposure', 'for', 'anyone', 'who', 'attended', 'five', 'churches', 'took', 'communion', 'Bishop', 'John', 'Folda', '(', 'pictured', ')', 'State', 'Immunization', 'Program', 'Manager', 'Molly', 'Howell', 'says', 'risk', 'is', 'low', 'but', 'officials', 'feel', 'it', \"'s\", 'important', 'alert', 'people', 'possible', 'diocese', 'announced', 'on', 'Monday', 'that', 'taking', 'time', 'off', 'after', 'being', 'diagnosed', 'with', 'he', 'contracted', 'infection', 'through', 'contaminated', 'food', 'while', 'attending', 'a', 'conference', 'newly', 'ordained', 'bishops', 'Italy', 'last', 'month', 'Symptoms', 'include', 'fever', 'tiredness', 'loss', 'appetite', 'nausea', 'abdominal', 'discomfort', 'where', 'located', 'CNN', '--', 'Ralph', 'Mata', 'was', 'internal', 'affairs', 'lieutenant', 'Miami-Dade', 'Police', 'working', 'division', 'investigates', 'allegations', 'wrongdoing', 'by', 'cops', 'Outside', 'office', 'authorities', 'allege', '45-year-old', 'longtime', 'officer', 'worked', 'drug', 'trafficking', 'organization', 'help', 'plan', 'murder', 'plot', 'get', 'guns', 'criminal', 'complaint', 'unsealed', 'U.S.', 'District', 'Court', 'New', 'Jersey', 'Tuesday', 'accuses', 'also', 'known', 'as', '``', 'Milk', 'Man', \"''\", 'using', 'his', 'role', 'police', 'exchange', 'money', 'gifts', 'including', 'Rolex', 'watch', 'In', 'one', 'instance', 'alleges', 'arranged', 'pay', 'two', 'assassins', 'kill', 'rival', 'dealers', 'killers', 'would', 'pose', 'pulling', 'over', 'their', 'targets', 'before', 'shooting', 'them', 'according', 'Ultimately', 'decided', 'not', 'move', 'forward', 'still', 'received', 'payment', 'setting', 'up', 'meetings', 'federal', 'prosecutors', 'said', 'statement', 'used', 'badge', 'purchase', 'weapons', 'traffickers', 'then', 'contacts', 'at', 'airport', 'transport', 'carry-on', 'luggage', 'trips', 'from', 'Miami', 'Dominican', 'Republic', 'documents', 'released', 'investigators', 'do', 'specify', 'name', 'which', 'allegedly', 'conspired', 'been', 'importing', 'narcotics', 'places', 'such', 'Ecuador', 'hiding', 'inside', 'shipping', 'containers', 'containing', 'pallets', 'produce', 'bananas', 'distributing', 'elsewhere', 'Authorities', 'arrested', 'Gardens', 'Florida', 'It', 'immediately', 'clear', 'whether', 'attorney', 'could', 'be', 'reached', 'comment', 'since', '1992', 'directing', 'investigations', 'K-9', 'unit', 'International', 'Airport', 'Since', 'March', '2010', 'had', 'faces', 'charges', 'aiding', 'abetting', 'conspiracy', 'distribute', 'cocaine', 'conspiring', 'engaging', 'monetary', 'transactions', 'property', 'derived', 'specified', 'unlawful', 'activity', 'He', 'scheduled', 'appear', 'court', 'Wednesday', 'If', 'convicted', 'face', 'life', 'prison', 'Suzanne', 'Presto', 'contributed', 'this', 'report', 'drunk', 'driver', 'killed', 'young', 'woman', 'head-on', 'crash', 'checking', 'mobile', 'phone', 'jailed', 'six', 'years', 'Craig', 'Eccleston-Todd', '27', 'driving', 'home', 'night', 'pub', 'when', 'text', 'message', 'As', 'reading', 'or', 'replying', 'veered', 'across', 'road', 'round', 'bend', 'smashed', 'into', 'Rachel', 'Titley', '’', 's', 'car', 'coming', 'other', 'way', 'left', 'crashed', 'driven', '28', 'right', 'She', 'died', 'later', 'her', 'injuries', 'place', 'Mr', 'barely', 'recognisable', 'least', 'three', 'four', 'pints', 'beer', 'getting', 'behind', 'wheel', 'found', 'guilty', 'causing', 'death', 'dangerous', 'Portsmouth', 'Crown', 'yesterday', 'Miss', '28-year-old', 'solicitor', 'clerk', 'Cowes', 'Isle', 'Wight', 'spent', 'evening', 'friends', 'any', 'alcohol', 'responsibly', 'there', '‘', 'nothing', 'she', 'have', 'done', 'avoid', 'collision', 'they', 'added', 'Lindsay', 'Pennell', 'prosecuting', 'resulted', 'tragic', 'avoided', 'decision', 'pick', 'whilst', 'either', 'so', 'distracted', 'failed', 'negotiate', 'left-hand', 'crossing', 'central', 'white', 'line', 'path', 'oncoming', 'pulled', 'wreckage', 'Daihatsu', 'Cuore', 'hospital', '[', ']', 'bright', 'future', 'ahead', 'returning', 'having', 'enjoyable', 'contact', 'got', 'confirm', 'arrived', 'safely', 'Her', 'sadly', 'never', 'heard', 'parted', 'company', 'these', 'circumstances', 'reiterates', 'danger', 'hand-held', 'driving.', 'were', 'unable', 'take', 'breath', 'blood', 'tests', 'several', 'hours', 'accident', 'only', 'marginally', 'under', 'drink-drive', 'limit', 'judge', 'agreed', 'red', 'Citroen', 'hit', 'blue', 'near', 'Yarmouth', '11', 'His', 'records', 'showed', 'texting', 'around', 'PC', 'Mark', 'Furse', 'Hampshire', 'constabulary', 'serious', 'investigation', \"'Our\", 'thoughts', 'are', 'family', 'out', 'Shalfleet', 'cost', \"'Mr\", 'work', 'met', 'drank', 'lager', \"n't\", 'long', 'return', 'occurred', '9.30pm', \"'We\", 'able', 'him', 'although', 'taken', 'we', 'maintain', 'summing', 'today', \"'The\", 'analysis', 'highly', 'likely', 'control', \"'\", 'following', 'trial', 'will', 'now', 'spend', 'bars', 'lost', 'forever', 'I', 'hope', 'make', 'think', 'twice', 'drinking', 'once', \"'re\", 'dangers', 'drink', 'obvious', 'Those', 'continue', 'spending', 'substantial', 'This', 'case', 'highlights', 'just', 'how', 'consequences', 'committing', 'offences', 'can', 'ever', 're', 'be.', 'Newport', 'disqualified', 'eight', 'complete', 'extended', 're-test', 'With', 'breezy', 'sweep', 'pen', 'President', 'Vladimir', 'Putin', 'wrote', 'new', 'chapter', 'Crimea', 'turbulent', 'history', 'region', 'returned', 'Russian', 'domain', 'Sixty', 'prior', 'Ukraine', 'breakaway', 'peninsula', 'signed', 'away', 'swiftly', 'Soviet', 'leader', 'Nikita', 'Khrushchev', 'But', 'dealing', 'blatant', 'land', 'grab', 'its', 'eastern', 'flank', 'wo', 'anywhere', 'quick', 'easy', 'Europe', '28-member', 'union', 'Because', 'unlike', 'rushed', 'referendum', 'everyone', 'say', 'After', 'initially', 'slapping', 'visa', 'restrictions', 'asset', 'freezes', 'limited', 'number', 'little', 'politicians', 'military', 'men', 'facing', 'urgent', 'calls', 'widen', 'scope', 'measures', 'target', 'business', 'community', 'particular', 'logic', 'those', 'run', 'Russia', 'own', 'essentially', 'sides', 'coin', 'Alexei', 'Navalny', 'one-time', 'Moscow', 'mayoral', 'contender', 'house', 'arrest', 'opposing', 'current', 'regime', 'called', 'leaders', 'ban', 'personal', 'banker', 'Chelsea', 'Football', 'Club', 'owner', 'Roman', 'Abramovich', 'keeping', 'loved', 'ones', 'abroad', 'Asset', 'especially', 'palatable', 'options', 'EU', 'because', 'rolled', 'discretionary', 'basis', 'without', 'requiring', 'cumbersome', 'legal', 'procedures', 'recourse', 'fact', 'cancels', 'visas', 'does', 'like', 'all', 'Just', 'look', 'Hermitage', 'Capital', 'founder', 'Bill', 'Browder', 'both', 'entry', 'Moscow-based', '2005', 'dare', 'go', 'back', 'banned', 'adoption', 'orphans', 'Americans', 'retaliation', 'US', 'implementation', 'anti-corruption', 'law', 'named', 'Sergei', 'Magnitsky', 'lawyer', 'year', 'detention', 'center', 'apparently', 'beaten', 'Yet', 'playing', \"'money\", 'talks', 'card', 'must', 'ready', 'action', 'walks', 'accept', 'sanctions', 'two-way', 'street', 'hurt', 'Targeting', 'peripatetic', 'sapping', 'tenuous', 'support', 'And', 'strategy', 'might', 'turn', 'silver', 'lining', 'awarding', 'countries', 'chance', 'finally', 'deal', 'some', 'more', 'unpleasant', 'patronage', 'laundering', 'corruption', 'inflated', 'prize', 'assets', 'London', 'Picasso', 'paintings', 'Where', 'should', 'hold', 'fire', 'though', 'trade', 'Two', 'decades', 'post-Soviet', 'rapprochement', 'almost']\n",
    "vocab_size = 2048\n",
    "# vocabulary = get_vocabulary([train_data, valid_data], ban_words, new_words, vocab_size, word_tokenize, 50)\n",
    "vocab_size = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2idx = {token: i for i, token in enumerate(vocabulary)}\n",
    "idx2token = {i: token for i, token in enumerate(vocabulary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(train_data, word_tokenize, token2idx, input_len=512, vocab=vocabulary)\n",
    "valid_dataset = TextDataset(valid_data, word_tokenize, token2idx, input_len=512, vocab=vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=512, shuffle=False)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=512, shuffle=False)\n",
    "dataloaders = {\n",
    "    'train': train_dataloader,\n",
    "    'valid': valid_dataloader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleLSTM(vocab_dim=vocab_size, embedding_dim=300, hidden_dim=128, num_layer=1)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "scheduler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = choose_device(model, device, parallel_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = train_loop(\n",
    "    model=model, \n",
    "    dataloader=dataloaders, \n",
    "    loss_func=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    parallel_type=parallel_type, \n",
    "    device=device, \n",
    "    early_stopping=10, \n",
    "    scheduler=scheduler, \n",
    "    epochs=3\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = test(\n",
    "    best_model, \n",
    "    prompt, \n",
    "    word_tokenize,\n",
    "    token2idx, \n",
    "    idx2token, \n",
    "    vocabulary, \n",
    "    device, \n",
    "    generation_len=100, \n",
    "    T=1, \n",
    "    use_argmax=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multi layer LSTM with char tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ban_words = ['\\t', '\\n', '\\u2009', '\\xa0', '|', '´', '’',  '‘', '–', 'ñ', 'â' , '“', '½', 'Î', 'í', 'ó', 'Â', 'Ã', 'ﬁ', '¿', '•', 'É', 'è', 'š', 'ū', 'ė', 'č', '¢', 'é', 'ô']\n",
    "new_words = ['<EOS>', '<BOS>', '<UNK>']\n",
    "vocabulary = ['<EOS>', '<BOS>', '<UNK>', 'B', 'y', ' ', '.', 'A', 's', 'o', 'c', 'i', 'a', 't', 'e', 'd', 'P', 'r', 'U', 'L', 'I', 'S', 'H', 'E', 'D', ':', '1', '4', 'T', ',', '2', '5', 'O', 'b', '0', '3', '6', 'h', 'p', 'f', 'F', 'g', 'C', 'l', 'n', 'N', 'k', 'x', 'u', 'm', 'G', 'J', 'w', 'v', '(', ')', 'z', 'M', \"'\", '-', 'R', '\"', '9', 'K', 'W', 'j', '7', '8', '[', ']', 'Y', 'q', 'V', '$', '%', 'Q', '£', '/', '!', 'Z', '¬', '#', ';', '©', '&', '@', '_', '?', '*', 'X', '—', '+', '…', '»', '”', '€', '<', '{', '}']\n",
    "vocab_size = 99\n",
    "# vocabulary =  get_vocabulary([train_data, valid_data], ban_words, new_words, vocab_size, word_tokenize, 0)\n",
    "vocab_size = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2idx = {token: i for i, token in enumerate(vocabulary)}\n",
    "idx2token = {i: token for i, token in enumerate(vocabulary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(train_data, char_tokenize, token2idx, input_len=512, vocab=vocabulary)\n",
    "valid_dataset = TextDataset(valid_data, char_tokenize, token2idx, input_len=512, vocab=vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=1024, shuffle=False)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=1024, shuffle=False)\n",
    "dataloaders = {\n",
    "    'train': train_dataloader,\n",
    "    'valid': valid_dataloader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleLSTM(vocab_dim=vocab_size, embedding_dim=300, hidden_dim=128, num_layer=5)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "scheduler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = choose_device(model, device, parallel_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = train_loop(\n",
    "    model=model, \n",
    "    dataloader=dataloaders, \n",
    "    loss_func=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    parallel_type=parallel_type, \n",
    "    device=device, \n",
    "    early_stopping=10, \n",
    "    scheduler=scheduler, \n",
    "    epochs=3\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text3 = test(\n",
    "    best_model, \n",
    "    prompt, \n",
    "    char_tokenize, \n",
    "    token2idx, \n",
    "    idx2token, \n",
    "    vocabulary, \n",
    "    device, \n",
    "    generation_len=100, \n",
    "    T=1, \n",
    "    use_argmax=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multi layer LSTM with word tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ban_words = ['\\t', '\\n', '\\u2009', '\\xa0', '(CNN)']\n",
    "new_words = ['<EOS>', '<BOS>', '<UNK>']\n",
    "vocabulary = ['<EOS>', '<BOS>', '<UNK>', 'By', '.', 'Associated', 'Press', 'PUBLISHED', ':', '14:11', 'EST', ',', '25', 'October', '2013', '|', 'UPDATED', '15:36', 'The', 'bishop', 'of', 'the', 'Fargo', 'Catholic', 'Diocese', 'in', 'North', 'Dakota', 'has', 'exposed', 'potentially', 'hundreds', 'church', 'members', 'Grand', 'Forks', 'and', 'Jamestown', 'to', 'hepatitis', 'A', 'virus', 'late', 'September', 'early', 'state', 'Health', 'Department', 'issued', 'an', 'advisory', 'exposure', 'for', 'anyone', 'who', 'attended', 'five', 'churches', 'took', 'communion', 'Bishop', 'John', 'Folda', '(', 'pictured', ')', 'State', 'Immunization', 'Program', 'Manager', 'Molly', 'Howell', 'says', 'risk', 'is', 'low', 'but', 'officials', 'feel', 'it', \"'s\", 'important', 'alert', 'people', 'possible', 'diocese', 'announced', 'on', 'Monday', 'that', 'taking', 'time', 'off', 'after', 'being', 'diagnosed', 'with', 'he', 'contracted', 'infection', 'through', 'contaminated', 'food', 'while', 'attending', 'a', 'conference', 'newly', 'ordained', 'bishops', 'Italy', 'last', 'month', 'Symptoms', 'include', 'fever', 'tiredness', 'loss', 'appetite', 'nausea', 'abdominal', 'discomfort', 'where', 'located', 'CNN', '--', 'Ralph', 'Mata', 'was', 'internal', 'affairs', 'lieutenant', 'Miami-Dade', 'Police', 'working', 'division', 'investigates', 'allegations', 'wrongdoing', 'by', 'cops', 'Outside', 'office', 'authorities', 'allege', '45-year-old', 'longtime', 'officer', 'worked', 'drug', 'trafficking', 'organization', 'help', 'plan', 'murder', 'plot', 'get', 'guns', 'criminal', 'complaint', 'unsealed', 'U.S.', 'District', 'Court', 'New', 'Jersey', 'Tuesday', 'accuses', 'also', 'known', 'as', '``', 'Milk', 'Man', \"''\", 'using', 'his', 'role', 'police', 'exchange', 'money', 'gifts', 'including', 'Rolex', 'watch', 'In', 'one', 'instance', 'alleges', 'arranged', 'pay', 'two', 'assassins', 'kill', 'rival', 'dealers', 'killers', 'would', 'pose', 'pulling', 'over', 'their', 'targets', 'before', 'shooting', 'them', 'according', 'Ultimately', 'decided', 'not', 'move', 'forward', 'still', 'received', 'payment', 'setting', 'up', 'meetings', 'federal', 'prosecutors', 'said', 'statement', 'used', 'badge', 'purchase', 'weapons', 'traffickers', 'then', 'contacts', 'at', 'airport', 'transport', 'carry-on', 'luggage', 'trips', 'from', 'Miami', 'Dominican', 'Republic', 'documents', 'released', 'investigators', 'do', 'specify', 'name', 'which', 'allegedly', 'conspired', 'been', 'importing', 'narcotics', 'places', 'such', 'Ecuador', 'hiding', 'inside', 'shipping', 'containers', 'containing', 'pallets', 'produce', 'bananas', 'distributing', 'elsewhere', 'Authorities', 'arrested', 'Gardens', 'Florida', 'It', 'immediately', 'clear', 'whether', 'attorney', 'could', 'be', 'reached', 'comment', 'since', '1992', 'directing', 'investigations', 'K-9', 'unit', 'International', 'Airport', 'Since', 'March', '2010', 'had', 'faces', 'charges', 'aiding', 'abetting', 'conspiracy', 'distribute', 'cocaine', 'conspiring', 'engaging', 'monetary', 'transactions', 'property', 'derived', 'specified', 'unlawful', 'activity', 'He', 'scheduled', 'appear', 'court', 'Wednesday', 'If', 'convicted', 'face', 'life', 'prison', 'Suzanne', 'Presto', 'contributed', 'this', 'report', 'drunk', 'driver', 'killed', 'young', 'woman', 'head-on', 'crash', 'checking', 'mobile', 'phone', 'jailed', 'six', 'years', 'Craig', 'Eccleston-Todd', '27', 'driving', 'home', 'night', 'pub', 'when', 'text', 'message', 'As', 'reading', 'or', 'replying', 'veered', 'across', 'road', 'round', 'bend', 'smashed', 'into', 'Rachel', 'Titley', '’', 's', 'car', 'coming', 'other', 'way', 'left', 'crashed', 'driven', '28', 'right', 'She', 'died', 'later', 'her', 'injuries', 'place', 'Mr', 'barely', 'recognisable', 'least', 'three', 'four', 'pints', 'beer', 'getting', 'behind', 'wheel', 'found', 'guilty', 'causing', 'death', 'dangerous', 'Portsmouth', 'Crown', 'yesterday', 'Miss', '28-year-old', 'solicitor', 'clerk', 'Cowes', 'Isle', 'Wight', 'spent', 'evening', 'friends', 'any', 'alcohol', 'responsibly', 'there', '‘', 'nothing', 'she', 'have', 'done', 'avoid', 'collision', 'they', 'added', 'Lindsay', 'Pennell', 'prosecuting', 'resulted', 'tragic', 'avoided', 'decision', 'pick', 'whilst', 'either', 'so', 'distracted', 'failed', 'negotiate', 'left-hand', 'crossing', 'central', 'white', 'line', 'path', 'oncoming', 'pulled', 'wreckage', 'Daihatsu', 'Cuore', 'hospital', '[', ']', 'bright', 'future', 'ahead', 'returning', 'having', 'enjoyable', 'contact', 'got', 'confirm', 'arrived', 'safely', 'Her', 'sadly', 'never', 'heard', 'parted', 'company', 'these', 'circumstances', 'reiterates', 'danger', 'hand-held', 'driving.', 'were', 'unable', 'take', 'breath', 'blood', 'tests', 'several', 'hours', 'accident', 'only', 'marginally', 'under', 'drink-drive', 'limit', 'judge', 'agreed', 'red', 'Citroen', 'hit', 'blue', 'near', 'Yarmouth', '11', 'His', 'records', 'showed', 'texting', 'around', 'PC', 'Mark', 'Furse', 'Hampshire', 'constabulary', 'serious', 'investigation', \"'Our\", 'thoughts', 'are', 'family', 'out', 'Shalfleet', 'cost', \"'Mr\", 'work', 'met', 'drank', 'lager', \"n't\", 'long', 'return', 'occurred', '9.30pm', \"'We\", 'able', 'him', 'although', 'taken', 'we', 'maintain', 'summing', 'today', \"'The\", 'analysis', 'highly', 'likely', 'control', \"'\", 'following', 'trial', 'will', 'now', 'spend', 'bars', 'lost', 'forever', 'I', 'hope', 'make', 'think', 'twice', 'drinking', 'once', \"'re\", 'dangers', 'drink', 'obvious', 'Those', 'continue', 'spending', 'substantial', 'This', 'case', 'highlights', 'just', 'how', 'consequences', 'committing', 'offences', 'can', 'ever', 're', 'be.', 'Newport', 'disqualified', 'eight', 'complete', 'extended', 're-test', 'With', 'breezy', 'sweep', 'pen', 'President', 'Vladimir', 'Putin', 'wrote', 'new', 'chapter', 'Crimea', 'turbulent', 'history', 'region', 'returned', 'Russian', 'domain', 'Sixty', 'prior', 'Ukraine', 'breakaway', 'peninsula', 'signed', 'away', 'swiftly', 'Soviet', 'leader', 'Nikita', 'Khrushchev', 'But', 'dealing', 'blatant', 'land', 'grab', 'its', 'eastern', 'flank', 'wo', 'anywhere', 'quick', 'easy', 'Europe', '28-member', 'union', 'Because', 'unlike', 'rushed', 'referendum', 'everyone', 'say', 'After', 'initially', 'slapping', 'visa', 'restrictions', 'asset', 'freezes', 'limited', 'number', 'little', 'politicians', 'military', 'men', 'facing', 'urgent', 'calls', 'widen', 'scope', 'measures', 'target', 'business', 'community', 'particular', 'logic', 'those', 'run', 'Russia', 'own', 'essentially', 'sides', 'coin', 'Alexei', 'Navalny', 'one-time', 'Moscow', 'mayoral', 'contender', 'house', 'arrest', 'opposing', 'current', 'regime', 'called', 'leaders', 'ban', 'personal', 'banker', 'Chelsea', 'Football', 'Club', 'owner', 'Roman', 'Abramovich', 'keeping', 'loved', 'ones', 'abroad', 'Asset', 'especially', 'palatable', 'options', 'EU', 'because', 'rolled', 'discretionary', 'basis', 'without', 'requiring', 'cumbersome', 'legal', 'procedures', 'recourse', 'fact', 'cancels', 'visas', 'does', 'like', 'all', 'Just', 'look', 'Hermitage', 'Capital', 'founder', 'Bill', 'Browder', 'both', 'entry', 'Moscow-based', '2005', 'dare', 'go', 'back', 'banned', 'adoption', 'orphans', 'Americans', 'retaliation', 'US', 'implementation', 'anti-corruption', 'law', 'named', 'Sergei', 'Magnitsky', 'lawyer', 'year', 'detention', 'center', 'apparently', 'beaten', 'Yet', 'playing', \"'money\", 'talks', 'card', 'must', 'ready', 'action', 'walks', 'accept', 'sanctions', 'two-way', 'street', 'hurt', 'Targeting', 'peripatetic', 'sapping', 'tenuous', 'support', 'And', 'strategy', 'might', 'turn', 'silver', 'lining', 'awarding', 'countries', 'chance', 'finally', 'deal', 'some', 'more', 'unpleasant', 'patronage', 'laundering', 'corruption', 'inflated', 'prize', 'assets', 'London', 'Picasso', 'paintings', 'Where', 'should', 'hold', 'fire', 'though', 'trade', 'Two', 'decades', 'post-Soviet', 'rapprochement', 'almost']\n",
    "vocab_size = 2048\n",
    "# vocabulary = get_vocabulary([train_data, valid_data], ban_words, new_words, vocab_size, word_tokenize, 50)\n",
    "vocab_size = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2idx = {token: i for i, token in enumerate(vocabulary)}\n",
    "idx2token = {i: token for i, token in enumerate(vocabulary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(train_data, char_tokenize, token2idx, input_len=384, vocab=vocabulary)\n",
    "valid_dataset = TextDataset(valid_data, char_tokenize, token2idx, input_len=384, vocab=vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=512, shuffle=False)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=512, shuffle=False)\n",
    "dataloaders = {\n",
    "    'train': train_dataloader,\n",
    "    'valid': valid_dataloader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleLSTM(vocab_dim=vocab_size, embedding_dim=300, hidden_dim=128, num_layer=5)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "scheduler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = choose_device(model, device, parallel_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = train_loop(\n",
    "    model=model, \n",
    "    dataloader=dataloaders, \n",
    "    loss_func=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    parallel_type=parallel_type, \n",
    "    device=device, \n",
    "    early_stopping=10, \n",
    "    scheduler=scheduler, \n",
    "    epochs=3\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text4 = test(\n",
    "    best_model, \n",
    "    prompt, \n",
    "    char_tokenize, \n",
    "    token2idx, \n",
    "    idx2token, \n",
    "    vocabulary, \n",
    "    device, \n",
    "    generation_len=100, \n",
    "    T=1, \n",
    "    use_argmax=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
